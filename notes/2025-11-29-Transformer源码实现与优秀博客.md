# Transformeræºç å®ç°ä¸ä¼˜ç§€åšå®¢


## ğŸ“˜ ä¼˜ç§€åšå®¢

* [Harvard NLP Annotated Transformerï¼ˆè¶…ç»å…¸å›¾æ–‡è®²è§£ è¶…çº§æ¨èï¼‰](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
  
* [The Illustrated Transformerï¼ˆå¯è§†åŒ–è®²è§£ï¼Œå…¥é—¨å¿…è¯»ï¼‰](https://jalammar.github.io/illustrated-transformer/)

## ğŸ§© æºç å®ç°

* [Annotated Transformerï¼ˆHarvard NLP å®˜æ–¹ä»£ç ï¼‰](https://github.com/harvardnlp/annotated-transformer)

* [Tensor2Tensorï¼ˆGoogle æ—©æœŸå®˜æ–¹å®ç°ï¼‰](https://github.com/tensorflow/tensor2tensor)
  
* [Attention Is All You Need - PyTorch å¤ç°](https://github.com/jadore801120/attention-is-all-you-need-pytorch)
  
* [HuggingFace Transformersï¼ˆä¸»æµåº“ï¼Œå·¥ä¸šçº§å®ç°ï¼‰](https://github.com/huggingface/transformers)
  
---

# Transformer åƒé€è®¡åˆ’


## 1) å…¨ç½‘é«˜è´¨é‡èµ„æ–™ç´¢å¼•ï¼ˆæŒ‰â€œåƒé€é¡ºåºâ€ï¼‰

1. **åŸè®ºæ–‡ï¼šTransformerä»0åˆ°1**
   Vaswani et al., *Attention Is All You Need*ï¼ˆæ¶æ„ã€Scaled Dot-Product Attentionã€å¤šå¤´ã€ä½ç½®ç¼–ç ã€maskç­‰éƒ½åœ¨è¿™é‡Œï¼‰([arXiv][1])
2. **é€è¡Œå®ç°ç‰ˆï¼šæŠŠè®ºæ–‡å˜æˆå¯è¿è¡Œä»£ç **ï¼ˆå¼ºçƒˆæ¨èï¼‰
   Harvard NLP *The Annotated Transformer*ï¼šæŠŠå…³é”®æ¨¡å—æ‹†å¼€ã€é…ä»£ç ä¸è§£é‡Š([nlp.seas.harvard.edu][2])
3. **æœ€å¼ºå¯è§†åŒ–ç›´è§‰ï¼šæŠŠQ/K/Vå’Œå¤šå¤´â€œçœ‹è§â€**
   Jay Alammar *The Illustrated Transformer*([jalammar.github.io][3])
4. **æ³¨æ„åŠ›æœºåˆ¶çš„â€œå‰ä¼ â€ï¼šä¸ºä»€ä¹ˆéœ€è¦attention**

   * Bahdanau additive attentionï¼šç¼“è§£â€œå›ºå®šé•¿åº¦å‘é‡ç“¶é¢ˆâ€ã€åšè½¯å¯¹é½([arXiv][4])
   * Luong attentionï¼šglobal/localã€ä¸åŒæ‰“åˆ†å‡½æ•°ä½“ç³»åŒ–æ€»ç»“([arXiv][5])
5. **Transformeré‡Œä¸¤ä¸ªâ€œè®­ç»ƒç¨³å®šæ€§ç¥å™¨â€çš„æ¥æº**

   * LayerNormï¼šä¸ä¾èµ–batchç»Ÿè®¡ã€è®­ç»ƒ/æ¨ç†ä¸€è‡´([arXiv][6])
   * Residualï¼ˆæ®‹å·®æ€æƒ³ï¼‰ï¼šæ·±ç½‘ç»œæ›´æ˜“ä¼˜åŒ–([arXiv][7])
6. **åº”ç”¨ä¾§é‡Œç¨‹ç¢‘ï¼šEncoder-onlyçš„ä»£è¡¨ï¼ˆç†è§£mask/åŒå‘çš„æ„ä¹‰ï¼‰**
   BERTï¼šæ·±åº¦åŒå‘Transformerè¡¨ç¤ºå­¦ä¹ ([arXiv][8])

---

## 2) æ³¨æ„åŠ›æœºåˆ¶ï¼šæŠŠå®ƒâ€œåƒé€â€éœ€è¦æŠ“ä½çš„3ä¸ªæœ¬è´¨

### æœ¬è´¨Aï¼šAttention = **å†…å®¹å¯»å€ï¼ˆcontent-based addressingï¼‰**

ç»™å®šä¸€ä¸ªâ€œæˆ‘ç°åœ¨è¦æ‰¾ä»€ä¹ˆâ€çš„**Query**ï¼Œå»ä¸€å †â€œå¯è¢«åŒ¹é…çš„ç´¢å¼•â€**Key**é‡Œç®—ç›¸ä¼¼åº¦ï¼Œå†å¯¹å¯¹åº”çš„â€œä¿¡æ¯å†…å®¹â€**Value**åšåŠ æƒå¹³å‡ã€‚

> ç›´è§‰ï¼šåƒåœ¨è®°å¿†åº“é‡Œç”¨Queryå»æ£€ç´¢Keyï¼Œæ£€ç´¢åˆ°çš„æƒé‡å†ç”¨æ¥æ±‡æ€»Valueã€‚

---

### æœ¬è´¨Bï¼šScaled Dot-Product Attention çš„æ•°å­¦éª¨æ¶

Transformerç”¨çš„æ˜¯ï¼ˆç¼©æ”¾ï¼‰ç‚¹ç§¯æ³¨æ„åŠ›ï¼š
[
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{mask}\right)V
]
è¿™æ˜¯è®ºæ–‡çš„æ ¸å¿ƒå…¬å¼ä¹‹ä¸€([arXiv][1])

**é€é¡¹åƒé€ï¼š**

* (QK^\top)ï¼šæ¯ä¸ªqueryå¯¹æ‰€æœ‰keyæ‰“åˆ†ï¼ˆç›¸ä¼¼åº¦çŸ©é˜µï¼Œå½¢çŠ¶é€šå¸¸æ˜¯ `T_q Ã— T_k`ï¼‰
* `softmax`ï¼šæŠŠç›¸ä¼¼åº¦å˜æˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰
* `mask`ï¼šæŠŠâ€œä¸å…è®¸çœ‹çš„ä½ç½®â€åŠ åˆ° (-\infty)ï¼Œsoftmaxåæƒé‡â‰ˆ0ï¼ˆåé¢ç»†è®²ï¼‰
* ä¹˜ (V)ï¼šå¯¹valueåšåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°â€œèåˆä¸Šä¸‹æ–‡â€çš„è¾“å‡º

**ä¸ºä»€ä¹ˆè¦é™¤ (\sqrt{d_k})**ï¼šç‚¹ç§¯éšç»´åº¦å¢å¤§æ–¹å·®å˜å¤§ï¼Œsoftmaxæ›´å®¹æ˜“é¥±å’Œï¼ˆæ¢¯åº¦å°ï¼‰ï¼Œç¼©æ”¾èƒ½ç¨³å®šè®­ç»ƒï¼›è¿™ä¹Ÿæ˜¯è®ºæ–‡æ˜ç¡®å†™å‡ºçš„è®¾è®¡ç‚¹([arXiv][1])

---

### æœ¬è´¨Cï¼šAdditive vs Dot-Productï¼ˆä½ ä¼šç»å¸¸åœ¨é¢è¯•/è®ºæ–‡é‡Œçœ‹åˆ°ï¼‰

* **Bahdanauï¼ˆadditiveï¼‰**ï¼šç”¨ä¸€ä¸ªå°MLPç®—ç›¸ä¼¼åº¦ï¼Œæ›´â€œè¡¨è¾¾åŠ›å¼ºâ€ï¼Œæ—©æœŸNMTå¸¸ç”¨([arXiv][4])
* **Dot-productï¼ˆmultiplicativeï¼‰**ï¼šçŸ©é˜µä¹˜æ³•æ›´é«˜æ•ˆã€é€‚åˆå¹¶è¡Œï¼›Transformeré€‰æ‹©å®ƒå¹¶åŠ äº†ç¼©æ”¾([arXiv][1])
  Harvardé‚£ç¯‡ä¹ŸæŠŠè¿™ä¸¤ç±»æ”¾åœ¨ä¸€èµ·å¯¹æ¯”è¿‡([nlp.seas.harvard.edu][2])

---

## 3) Transformerï¼šä½ è¦æŠŠå®ƒå½“æˆâ€œå¯é‡å¤å †å çš„ç§¯æœ¨â€

### 3.1 ä¸€å±‚Encoderé•¿ä»€ä¹ˆæ ·

ä¸€å±‚EncoderåŸºæœ¬æ˜¯ä¸¤å—ï¼š

1. **Multi-Head Self-Attention**
2. **Position-wise FFNï¼ˆé€ä½ç½®å‰é¦ˆç½‘ç»œï¼‰**
   æ¯å—å¤–é¢éƒ½æœ‰ **Residual + LayerNormï¼ˆAdd & Normï¼‰**([arXiv][1])

**å…³é”®ç‚¹ï¼šSelf-Attentioné‡Œ Qã€Kã€V éƒ½æ¥è‡ªåŒä¸€ä»½è¾“å…¥ (X)**ï¼ˆåªæ˜¯ä¹˜ä¸åŒçº¿æ€§å˜æ¢å¾—åˆ°ï¼‰ï¼š
[
Q=XW_Q,\quad K=XW_K,\quad V=XW_V
]
ï¼ˆHarvardå®ç°ç‰ˆæŠŠè¿™äº›å†™å¾—éå¸¸æ¸…æ¥šï¼‰([nlp.seas.harvard.edu][2])

---

### 3.2 Decoderä¸ºä»€ä¹ˆå¤šä¸€å—â€œCross-Attentionâ€

Decoderæ¯å±‚é€šå¸¸æ˜¯ä¸‰å—ï¼š

1. **Masked Multi-Head Self-Attention**ï¼ˆä¸èƒ½çœ‹æœªæ¥ï¼‰
2. **Cross-Attention**ï¼š**Qæ¥è‡ªdecoderå½“å‰éšçŠ¶æ€**ï¼Œ**K/Væ¥è‡ªencoderè¾“å‡º**ï¼ˆæŠŠæºåºåˆ—ä¿¡æ¯â€œå–â€è¿‡æ¥ï¼‰([arXiv][1])
3. **FFN**

è¿™å°±æ˜¯ç»å…¸encoder-decoder Transformerç¿»è¯‘æ¶æ„([arXiv][1])

---

## 4) Multi-Head Attentionï¼šä¸åªæ˜¯â€œå¤šåšå‡ æ¬¡attentionâ€

å¤šå¤´æ³¨æ„åŠ›åšçš„äº‹æ˜¯ï¼šæŠŠè¡¨ç¤ºç»´åº¦åˆ‡æˆå¤šä¸ªå­ç©ºé—´ï¼Œåœ¨ä¸åŒå­ç©ºé—´é‡Œåˆ†åˆ«åšattentionï¼Œå†æ‹¼èµ·æ¥ï¼š

[
\text{head}_i=\mathrm{Attention}(XW_Q^i, XW_K^i, XW_V^i)
]
[
\mathrm{MultiHead}(X)=\mathrm{Concat}(\text{head}_1,\dots,\text{head}_h)W_O
]
è¿™æ˜¯Transformerè¡¨è¾¾åŠ›çš„å…³é”®ä¹‹ä¸€([arXiv][1])

**ä½ çœŸæ­£è¦åƒé€çš„ç›´è§‰ï¼š**

* å•å¤´ï¼šåªèƒ½å­¦åˆ°ä¸€ç§â€œç›¸å…³æ€§åº¦é‡/èšåˆæ–¹å¼â€
* å¤šå¤´ï¼šèƒ½å¹¶è¡Œå­¦å¤šç§å…³ç³»ï¼ˆè¯­æ³•ä¾èµ–ã€æŒ‡ä»£ã€ä¸»é¢˜ä¸€è‡´æ€§â€¦ï¼‰ï¼Œæœ€åèåˆ

---

## 5) Maskï¼šTransformeré‡Œæœ€å®¹æ˜“â€œçœ‹æ‡‚ä½†å†™é”™â€çš„åœ°æ–¹

ä½ è‡³å°‘è¦åŒºåˆ†ä¸¤ç§maskï¼š

1. **Padding maskï¼ˆå¡«å……maskï¼‰**
   æŠŠpaddingä½ç½®å±è”½æ‰ï¼Œå¦åˆ™æ¨¡å‹ä¼šæŠŠâ€œPADâ€ä¹Ÿå½“æˆä¿¡æ¯ã€‚Harvardå®ç°é‡Œæœ‰å¾ˆæ¸…æ™°çš„maskæ„é€ æ–¹å¼([nlp.seas.harvard.edu][2])

2. **Causal / Subsequent maskï¼ˆå› æœmaskï¼‰**
   ç”¨åœ¨Decoder self-attentionï¼šä½ç½®tåªèƒ½çœ‹ (\le t) çš„tokenï¼Œä¿è¯è‡ªå›å½’ç”Ÿæˆçš„å› æœæ€§([arXiv][1])

---

## 6) ä½ç½®ç¼–ç ï¼šæ²¡æœ‰RNNä»¥åï¼Œâ€œé¡ºåºâ€ä»å“ªæ¥ï¼Ÿ

Transformeræ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæ‰€ä»¥å¿…é¡»æ˜¾å¼æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚åŸè®ºæ–‡ç”¨çš„æ˜¯**æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç **ï¼ˆä¹Ÿå¯å­¦ä¹ ä½ç½®embeddingï¼‰([arXiv][1])

ä½ è¦æŠ“ä½çš„è¦ç‚¹ï¼š

* **Self-attentionæœ¬èº«æ˜¯ç½®æ¢ä¸å˜çš„**ï¼ˆæ‰“ä¹±tokené¡ºåºï¼Œæ³¨æ„åŠ›è®¡ç®—å½¢å¼ä¸å˜ï¼‰ï¼Œæ‰€ä»¥å¿…é¡»åŠ ä½ç½®
* ä½ç½®ç¼–ç ç­‰ä»·äºå‘Šè¯‰æ¨¡å‹â€œç¬¬å‡ ä¸ªtokenâ€ï¼Œè®©æ³¨æ„åŠ›èƒ½è¡¨è¾¾ç›¸å¯¹/ç»å¯¹é¡ºåºå…³ç³»

---

## 7) ä½ æƒ³â€œå®Œå…¨åƒé€â€ï¼šç»™ä½ ä¸€å¥—æœ€æœ‰æ•ˆçš„è®­ç»ƒæ–¹å¼ï¼ˆä¸é æ­»è®°ï¼‰

### 7.1 6ä¸ªå¿…é¡»èƒ½æ‰‹å†™/å£è¿°çš„â€œæ£€æŸ¥ç‚¹â€

1. å†™å‡ºå¹¶è§£é‡Š attention å…¬å¼ï¼ˆå«maskï¼‰([arXiv][1])
2. è¯´æ¸… self-attn vs cross-attn çš„Q/K/Væ¥æº([nlp.seas.harvard.edu][2])
3. è¯´æ¸… multi-head çš„â€œä¸ºä»€ä¹ˆä¸æ˜¯å¤šæ­¤ä¸€ä¸¾â€([NeurIPS Proceedings][9])
4. è¯´æ¸…ä¸¤ç§maskå„è‡ªè§£å†³ä»€ä¹ˆé—®é¢˜([nlp.seas.harvard.edu][2])
5. è¯´æ¸… residual + layernorm ä¸ºä»€ä¹ˆèƒ½ç¨³å®šè®­ç»ƒ([nlp.seas.harvard.edu][2])
6. è¯´æ¸…ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ã€åŸè®ºæ–‡æ€ä¹ˆåš([arXiv][1])

### 7.2 3ä¸ªâ€œåšå®Œå°±ä¼šäº†â€çš„å®æˆ˜ä»»åŠ¡

* **ä»»åŠ¡1ï¼šä»é›¶å®ç°Scaled Dot-Product Attentionï¼ˆå¸¦maskï¼‰**
  è¾“å…¥éšæœºQ/K/Vï¼Œæ£€æŸ¥ï¼šmaskä½ç½®æƒé‡â‰ˆ0ï¼›softmaxæ¯è¡Œå’Œä¸º1ã€‚å‚è€ƒHarvardé€è¡Œå®ç°([nlp.seas.harvard.edu][2])
* **ä»»åŠ¡2ï¼šå®ç°Multi-Head Attentionå¹¶åšå½¢çŠ¶è‡ªæ£€**
  å¼ºåˆ¶è‡ªå·±å†™å‡ºæ¯ä¸€æ­¥å¼ é‡å½¢çŠ¶ï¼ˆ`B,T,d_model` â†’ `B,h,T,d_k` ç­‰ï¼‰ï¼Œæœ€èƒ½æ²»â€œçœ‹æ‡‚ä½†å†™ä¸å‡ºæ¥â€ã€‚([nlp.seas.harvard.edu][2])
* **ä»»åŠ¡3ï¼šè·‘ä¸€ä¸ªæœ€å°Transformer**
  ç”¨æå°æ•°æ®ï¼ˆcopy task / reverse taskï¼‰è®­ç»ƒåˆ°è¿‡æ‹Ÿåˆï¼›è¿™ä¼šè®©ä½ çœŸæ­£ç†è§£maskã€ä½ç½®ã€decoderè¾“å…¥å³ç§»ç­‰ç»†èŠ‚ã€‚Harvardé‚£ç¯‡å°±æœ‰è®­ç»ƒè„šæ‰‹æ¶æ€è·¯([nlp.seas.harvard.edu][2])

---

## 8) Transformerå®¶æ—ä¸€çœ¼çœ‹æ‡‚ï¼šä½ åœ¨å­¦çš„åˆ°åº•æ˜¯å“ªä¸€ç§ï¼Ÿ

* **Encoder-Decoder**ï¼šæœºå™¨ç¿»è¯‘/Seq2Seqï¼ˆåŸè®ºæ–‡ï¼‰([arXiv][1])
* **Encoder-only**ï¼šç†è§£å‹ä»»åŠ¡ï¼ˆBERTæ˜¯ä»£è¡¨ï¼‰([arXiv][8])
* **Decoder-only**ï¼šè‡ªå›å½’ç”Ÿæˆï¼ˆå¾ˆå¤šLLMå±äºè¿™ç±»ï¼›æ ¸å¿ƒæ˜¯causal maskï¼‰([arXiv][1])

---
[1]: https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com "Attention Is All You Need"
[2]: https://nlp.seas.harvard.edu/annotated-transformer/?utm_source=chatgpt.com "The Annotated Transformer - Harvard University"
[3]: https://jalammar.github.io/illustrated-transformer/?utm_source=chatgpt.com "The Illustrated Transformer â€“ Jay Alammar â€“ Visualizing machine ..."
[4]: https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com "Neural Machine Translation by Jointly Learning to Align and Translate"
[5]: https://arxiv.org/abs/1508.04025?utm_source=chatgpt.com "Effective Approaches to Attention-based Neural Machine Translation"
[6]: https://arxiv.org/abs/1607.06450?utm_source=chatgpt.com "Layer Normalization"
[7]: https://arxiv.org/abs/1512.03385?utm_source=chatgpt.com "Deep Residual Learning for Image Recognition"
[8]: https://arxiv.org/abs/1810.04805?utm_source=chatgpt.com "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
[9]: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf?utm_source=chatgpt.com "Attention Is All You Need - NeurIPS"


---

# Transformer ç›¸å…³ä¼˜ç§€è§†é¢‘è®²è§£

ä¸‹é¢æ˜¯æˆ‘å¸®ä½ â€œå…¨ç½‘æ·±åº¦æœâ€å‡ºæ¥ã€**çœŸè®²é€æ³¨æ„åŠ›æœºåˆ¶ + Transformer** çš„é«˜è´¨é‡è§†é¢‘æ¸…å•ï¼ˆä¸­è‹±éƒ½æœ‰ï¼‰ï¼Œå¹¶ç»™ä½ ä¸€æ¡**æœ€çœæ—¶é—´çš„è§‚çœ‹é¡ºåº**ã€‚æ¯ä¸ªæ¡ç›®åé¢éƒ½å¸¦å¯ç‚¹çš„æ¥æºã€‚

---

## æœ€æ¨èçš„è§‚çœ‹é¡ºåºï¼ˆç…§è¿™ä¸ªçœ‹ï¼Œæœ€å®¹æ˜“â€œåƒé€â€ï¼‰

1. **å…ˆå»ºç«‹ç›´è§‰ï¼ˆçœ‹æ‡‚ Q/K/Vã€softmaxã€multi-head åˆ°åº•åœ¨å¹²å˜›ï¼‰**

* 3Blue1Brownï¼š*Attention in transformers, step-by-step*ï¼ˆæŠŠæ³¨æ„åŠ›çŸ©é˜µæ€ä¹ˆæ¥çš„è®²å¾—ææ¸…æ¥šï¼Œå¼ºçƒˆæ¨èå…ˆçœ‹ï¼‰ ([YouTube][1])
* MIT 6.S191ï¼ˆ2025ç‰ˆï¼‰ï¼š*RNNs, Transformers, and Attention*ï¼ˆä»åºåˆ—å»ºæ¨¡ç—›ç‚¹ â†’ attention â†’ transformerï¼Œä½“ç³»å®Œæ•´ï¼‰ ([YouTube][2])

2. **å†æŠŠâ€œæ ‡å‡†Transformerâ€ç³»ç»ŸåŒ–ï¼ˆç»“æ„ã€maskã€ä½ç½®ç¼–ç ã€è®­ç»ƒç»†èŠ‚ï¼‰**

* Stanford CS224Nï¼ˆLecture 8ï¼‰ï¼š*Self-Attention and Transformers*ï¼ˆç»å…¸è¯¾ï¼Œæ¨å¯¼+ç»“æ„è®²å¾—å¾ˆæ­£ï¼‰ ([YouTube][3])
* Stanford CS224Nï¼ˆLecture 7ï¼‰ï¼š*Attention*ï¼ˆä¸“é—¨è®²attentionï¼Œé€‚åˆè¡¥é½åŸºç¡€ä¸ç»†èŠ‚ï¼‰ ([YouTube][4])

3. **æœ€åç”¨â€œè®ºæ–‡å¸¦è¯» + ä»£ç å®æˆ˜â€å®Œæˆé—­ç¯ï¼ˆçœŸæ­£åƒé€çš„å…³é”®ï¼‰**

* ææ²ï¼ˆBç«™ï¼‰ï¼š*Transformerè®ºæ–‡é€æ®µç²¾è¯»*ï¼ˆæŒ‰è®ºæ–‡é€æ®µè®²ï¼Œä¿¡æ¯å¯†åº¦æé«˜ï¼‰ ([å“”å“©å“”å“©][5])
* Andrej Karpathyï¼š*Letâ€™s build GPT from scratch*ï¼ˆä»é›¶å†™GPTï¼Œmaskã€embeddingã€è®­ç»ƒå¾ªç¯å…¨æ‰“é€šï¼‰ ([YouTube][6])

---

## ä¸­æ–‡å‘ï¼šéå¸¸é€‚åˆç³»ç»Ÿåƒé€ï¼ˆå…¥é—¨â†’è¿›é˜¶ï¼‰

* æå®æ¯…ï¼ˆBç«™ç›¸å…³åˆé›†ï¼‰ï¼š*è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸Transformerè¯¦è§£*ï¼ˆæ›´å‹å¥½ã€æ›´é€‚åˆå…¥é—¨å»ºç«‹æ¡†æ¶ï¼‰ ([å“”å“©å“”å“©][7])
* åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ï¼ˆææ²ï¼‰ï¼š*68 Transformer*ï¼ˆé…åˆå®ç°ä¸å·¥ç¨‹ç»†èŠ‚ï¼Œé€‚åˆâ€œä¼šå†™å‡ºæ¥â€ï¼‰ ([YouTube][8])
* ææ²ï¼ˆBç«™ï¼‰ï¼š*Transformerè®ºæ–‡é€æ®µç²¾è¯»*ï¼ˆè¿›é˜¶å¿…çœ‹ï¼Œåƒé€â€œä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡â€ï¼‰ ([å“”å“©å“”å“©][5])

---

## è‹±æ–‡å‘ï¼šå¤§å­¦è¯¾ç¨‹çº§åˆ«ï¼ˆæ·±åº¦ + è§†é‡ï¼‰

* Stanford CS25ï¼ˆTransformers Unitedï¼Œæ•´å¥—playlistï¼‰ï¼šè¦†ç›–NLP/CV/RL/å¯¹é½/æ£€ç´¢å¢å¼º/ä¸“å®¶æ··åˆç­‰ï¼ˆçœ‹å®Œè§†é‡ä¼šéå¸¸å¼€ï¼‰ ([YouTube][9])
* Stanford CS25ï¼ˆå«Karpathyçš„Intro to Transformersï¼‰ï¼šè®²Transformerç›´è§‰ä¸æ¼”åŒ–è„‰ç»œå¾ˆå¼º ([YouTube][9])
* Stanford CME295ï¼ˆ2025ï¼‰ï¼š*Transformers & LLMs*ï¼ˆæ›´è´´è¿‘â€œLLMæ—¶ä»£â€çš„è¯¾å ‚ç»“æ„ï¼Œå«attentionä¸transformeræ€»è§ˆï¼‰ ([YouTube][10])
* fast.aiï¼ˆLesson 24ï¼šAttention & Transformersï¼‰ï¼šåâ€œä»£ç ç›´è§‰+åº”ç”¨â€ï¼Œé€‚åˆè¡¥ä¸åŒè§†è§’ ([Practical Deep Learning for Coders][11])

---

## å¦‚æœä½ åªæƒ³æŒ‘â€œ3å¥—â€å°±åƒé€ï¼ˆæœ€çœæ—¶é—´ç‰ˆï¼‰

1. 3Blue1Brownï¼šæŠŠæ³¨æ„åŠ›æœºåˆ¶çœŸæ­£çœ‹æ‡‚ ([YouTube][1])
2. CS224N Lecture 8ï¼šç³»ç»ŸæŒæ¡Transformeræ ‡å‡†ç»“æ„ä¸è¦ç‚¹ ([YouTube][3])
3. Karpathyï¼šä»é›¶å†™GPTï¼ŒæŠŠmask/è®­ç»ƒ/æ¨ç†ç»†èŠ‚å…¨éƒ¨è½åœ° ([YouTube][6])

---

## ä½ â€œåƒé€â€çš„è‡ªæµ‹æ ‡å‡†ï¼ˆçœ‹å®Œè¦èƒ½åšåˆ°è¿™äº›ï¼‰

* èƒ½æ‰‹å†™/å£è¿°ï¼šScaled Dot-Product Attentionã€ä¸ºä»€ä¹ˆè¦é™¤ âˆšdã€ä¸ºä»€ä¹ˆmulti-headæœ‰æ•ˆ
* èƒ½è¯´æ¸…ï¼š**padding mask vs causal mask** å„è§£å†³ä»€ä¹ˆé—®é¢˜
* èƒ½ç”»å‡ºä¸€å±‚ï¼šEncoderå±‚ / Decoderå±‚ï¼Œå¹¶è¯´æ˜ self-attn / cross-attn çš„ Q/K/V åˆ†åˆ«æ¥è‡ªå“ªé‡Œ
* èƒ½è‡ªå·±å®ç°ä¸€ä¸ªæœ€å°Transformer/GPTï¼ˆå“ªæ€•å°æ•°æ®è¿‡æ‹Ÿåˆï¼‰

---

[1]: https://www.youtube.com/watch?v=eMlx5fFNoYc&utm_source=chatgpt.com "Attention in transformers, step-by-step | Deep Learning Chapter 6"
[2]: https://www.youtube.com/watch?v=GvezxUdLrEk&utm_source=chatgpt.com "MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention"
[3]: https://www.youtube.com/watch?v=LWMzyfvuehA&utm_source=chatgpt.com "Stanford CS224N NLP with Deep Learning | 2023 | Lecture 8 - YouTube"
[4]: https://www.youtube.com/watch?v=J7ruSOIzhrE&utm_source=chatgpt.com "Stanford CS224N: NLP w/ DL | Spring 2024 | Lecture 7 - YouTube"
[5]: https://www.bilibili.com/opus/586487984822265072?utm_source=chatgpt.com "Transformerè®ºæ–‡é€æ®µç²¾è¯»ã€è®ºæ–‡ç²¾è¯»ã€‘ - å“”å“©å“”å“©"
[6]: https://www.youtube.com/watch?v=kCc8FmEb1nY&utm_source=chatgpt.com "Let's build GPT: from scratch, in code, spelled out. - YouTube"
[7]: https://www.bilibili.com/list/3537113337301588?bvid=BV1r8nMz4EAj&oid=115269171155173&utm_source=chatgpt.com "æå®æ¯… | è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒTransformerè¯¦ç»†è§£æï¼Œç›®å‰æœ€åˆé€‚ ..."
[8]: https://www.youtube.com/watch?v=oIH17WP8Wko&utm_source=chatgpt.com "68 Transformerã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ã€‘ - YouTube"
[9]: https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&utm_source=chatgpt.com "Stanford CS25 - Transformers United - YouTube"
[10]: https://www.youtube.com/watch?v=Ub3GoFaUcds&utm_source=chatgpt.com "Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 1 ... - YouTube"
[11]: https://course.fast.ai/Lessons/lesson24.html?utm_source=chatgpt.com "24: Attention & transformers - Practical Deep Learning for Coders"


---

# Transformer ç›¸å…³ç»¼è¿°


ä¸‹é¢æŒ‰â€œ**æœ€å…¨æ€»è§ˆ** â†’ **é«˜æ•ˆ/é•¿ä¸Šä¸‹æ–‡** â†’ **è§†è§‰/å¤šæ¨¡æ€** â†’ **å‹ç¼©ä¸éƒ¨ç½²**â€ç»™ä½ ä¸€ç»„**å£ç¢‘+è¦†ç›–é¢+ä¿¡æ¯å¯†åº¦**éƒ½å¾ˆå¼ºçš„ Transformer/å˜ç§ç»¼è¿°ï¼ˆåŸºæœ¬éƒ½èƒ½å½“æ–‡çŒ®å…¥å£ç”¨ï¼‰ã€‚

## å¿…è¯»ï¼šTransformer å˜ç§å…¨æ™¯æ€»è§ˆï¼ˆæœ€æ¥è¿‘ä½ è¦çš„â€œæœ€å…¨â€ï¼‰

1. **A Survey of Transformers (arXiv:2106.04554)**
   ç»™äº†æ¯”è¾ƒç³»ç»Ÿçš„ **X-formers åˆ†ç±»æ³•**ï¼Œä»**ç»“æ„æ”¹é€ ã€é¢„è®­ç»ƒã€åº”ç”¨**ä¸‰ä¸ªè§†è§’æŠŠå¤§é‡å˜ç§ä¸²èµ·æ¥ï¼Œé€‚åˆä½œä¸ºâ€œæ€»ç›®å½•â€ã€‚ ([arXiv][1])

2. **Efficient Transformers: A Survey (arXiv:2009.06732)**
   ä¸“æ”»â€œ**ä¸ºä»€ä¹ˆ/æ€ä¹ˆæŠŠ Transformer åšé«˜æ•ˆ**â€ï¼šç¨€ç–ã€ä½ç§©ã€çº¿æ€§æ³¨æ„åŠ›ã€é•¿åºåˆ—ç­‰ç»å…¸è·¯çº¿éƒ½æœ‰æ¢³ç†ï¼Œé€‚åˆä½ è¦åƒé€å„ç§ç»“æ„ trick çš„â€œè„‰ç»œå›¾â€ã€‚ ([arXiv][2])

3. **A Historical Survey of Advances in Transformer Architectures (Applied Sciences, 2024)**
   åâ€œ**å†å²æ¼”è¿›è§†è§’**â€ï¼šä»æ—©æœŸ Transformer åˆ° LLM/ViT æ—¶ä»£çš„å…³é”®åˆ†å‰ä¸ä»£è¡¨ä½œï¼Œé€‚åˆæŠŠå‘å±•æ—¶é—´çº¿æ‹é¡ºã€‚ ([MDPI][3])

---

## æ³¨æ„åŠ›æœºåˆ¶ & é•¿åºåˆ—ï¼šæŠŠâ€œattention è¿™å¨â€åƒé€çš„ç»¼è¿°å…¥å£

4. **Efficient Attention Methods: Hardware-efficient, Sparse, Compact, and Linear Attention (PDF)**
   å¾ˆç¡¬æ ¸çš„â€œ**æ³¨æ„åŠ›ä¼˜åŒ–å¤§å…¨**â€ï¼ŒæŠŠæ–¹æ³•æŒ‰ **ç¡¬ä»¶å‹å¥½ / ç¨€ç– / KV å‹ç¼© / çº¿æ€§æ³¨æ„åŠ›**åšç»Ÿä¸€ taxonomyï¼Œè¿˜é…ç»Ÿä¸€åˆ†ææ¡†æ¶ï¼ˆæƒ³æŠŠ attention ç ”ç©¶çº¿ç´¢ä¸€æ¬¡æ€§æŒæ¡ï¼Œè¿™ç¯‡å¾ˆé¡¶ï¼‰ã€‚ 

5. **Advancing Transformer Architecture in Long-Context LLMs: A Comprehensive Survey (arXiv:2311.12351)**
   ä¸“é—¨è®² **é•¿ä¸Šä¸‹æ–‡**ï¼šä»é¢„è®­ç»ƒåˆ°æ¨ç†é˜¶æ®µçš„æ¶æ„å‡çº§ã€è¯„æµ‹æ•°æ®é›†/æŒ‡æ ‡ã€å·¥å…·é“¾ç­‰ï¼Œé€‚åˆâ€œé•¿ä¸Šä¸‹æ–‡èƒ½åŠ›â€è¿™æ¡ä¸»çº¿æ·±æŒ–ã€‚ ([arXiv][4])

---

## è§†è§‰ Transformer å˜ç§ï¼šCV æ–¹å‘æœ€æƒå¨çš„å…¥å£ä¹‹ä¸€

6. **Transformers in Vision: A Survey (arXiv:2101.01169)**
   è¦†ç›–å¾ˆå¹¿ï¼šåˆ†ç±»/æ£€æµ‹/åˆ†å‰²ã€ç”Ÿæˆã€å¤šæ¨¡æ€ã€è§†é¢‘ã€ä½å±‚è§†è§‰ã€3D ç­‰ï¼Œé€‚åˆæŠŠ ViT ç³»åˆ—ä¸ CV ä»»åŠ¡è„‰ç»œä¸€æ¬¡è¿èµ·æ¥ã€‚ ([arXiv][5])

---

## å‹ç¼©ã€æ¨ç†ä¸è½åœ°ï¼šæƒ³æŠŠâ€œå˜ç§â€ç†è§£åˆ°å·¥ç¨‹å±‚å¿…è¯»

7. **A Survey on Transformer Compression (arXiv:2402.05964)**
   Transformer ä¸“å±çš„å‹ç¼©è§†è§’ï¼šå‰ªæã€é‡åŒ–ã€è’¸é¦ã€ç»“æ„é«˜æ•ˆåŒ–ç­‰ï¼Œå¹¶ä¸”è¦†ç›– LLM/LVM åœºæ™¯ã€‚ ([arXiv][6])

8. **A Survey of Techniques for Optimizing Transformer Inference (arXiv:2307.07982)**
   æ›´åâ€œæ¨ç†é˜¶æ®µå…¨æ ˆä¼˜åŒ–â€ï¼šç®—æ³•å±‚ï¼ˆè’¸é¦/å‰ªæ/é‡åŒ–/NASâ€¦ï¼‰åˆ°ç¡¬ä»¶ä¸åŠ é€Ÿå™¨è®¾è®¡éƒ½æœ‰æ•´ç†ï¼Œé€‚åˆå·¥ç¨‹è½åœ°ã€‚ ([arXiv][7])

9. **From Transformers to LLMs: A Systematic Survey of Efficiency Considerations in NLP (arXiv:2406.16893, v2 2025)**
   NLP/LLM ä¾§çš„â€œæ•ˆç‡ç³»ç»Ÿç»¼è¿°â€ï¼Œè¦†ç›–åˆ° 2011â€“2025 çš„å¤§é‡å·¥ä½œï¼Œè¿˜æŠŠ **å¾®è°ƒã€æç¤ºå·¥ç¨‹ã€RAG** çš„æ•ˆç‡é—®é¢˜ä¹Ÿçº³å…¥è®¨è®ºã€‚ ([arXiv][8])

10. **A Survey on LLM Acceleration based on KV Cache Management (arXiv:2412.19442, TMLR 2025)**
    é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ ¸å¿ƒç“¶é¢ˆä¹‹ä¸€ï¼š**KV cache**ã€‚è¿™ç¯‡æŠŠ token/model/system ä¸‰å±‚ç­–ç•¥åšäº†æ¸…æ™°åˆ†ç±»ï¼Œè¿˜ç»™äº†åŸºå‡†ä¸è®ºæ–‡æ¸…å•ä»“åº“ã€‚ ([arXiv][9])

---

## å¦‚æœä½ æƒ³â€œæœ€å¿«åƒé€â€ï¼Œå»ºè®®é˜…è¯»é¡ºåº

* **å…ˆè¯»æ€»è§ˆ**ï¼š#1ï¼ˆå…¨æ™¯ taxonomyï¼‰â†’ #2ï¼ˆé«˜æ•ˆ X-formersï¼‰
* **å†æŠŠæ³¨æ„åŠ›æé€**ï¼š#4ï¼ˆattention ä¼˜åŒ–å¤§å…¨ï¼‰
* **æŒ‰ä½ æ–¹å‘é€‰åˆ†æ”¯**ï¼šCV å°± #6ï¼›é•¿ä¸Šä¸‹æ–‡å°± #5 + #10ï¼›å·¥ç¨‹éƒ¨ç½²å°± #7 + #8ï¼›NLP æ•ˆç‡ä¸RAGå°± #9

å¦‚æœä½ å‘Šè¯‰æˆ‘ä½ æ›´å…³æ³¨ï¼š**LLMï¼ˆé•¿ä¸Šä¸‹æ–‡/æ¨ç†ï¼‰** è¿˜æ˜¯ **CVï¼ˆViT ä½“ç³»ï¼‰**ï¼Œæˆ‘å¯ä»¥æŠŠä¸Šé¢è¿™å‡ ç¯‡å†ç»†åŒ–æˆâ€œæ¯ç¯‡è¯¥çœ‹å“ªäº›ç« èŠ‚ã€é…å¥—è¯¥è¡¥å“ªäº›ç»å…¸åŸè®ºæ–‡â€çš„ç²¾è¯»è·¯çº¿ã€‚

[1]: https://arxiv.org/abs/2106.04554 "[2106.04554] A Survey of Transformers"
[2]: https://arxiv.org/abs/2009.06732 "[2009.06732] Efficient Transformers: A Survey"
[3]: https://www.mdpi.com/2076-3417/14/10/4316 "A Historical Survey of Advances in Transformer Architectures | MDPI"
[4]: https://arxiv.org/abs/2311.12351 "[2311.12351] Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"
[5]: https://arxiv.org/abs/2101.01169 "[2101.01169] Transformers in Vision: A Survey"
[6]: https://arxiv.org/abs/2402.05964 "[2402.05964] A Survey on Transformer Compression"
[7]: https://arxiv.org/abs/2307.07982 "[2307.07982] A Survey of Techniques for Optimizing Transformer Inference"
[8]: https://arxiv.org/abs/2406.16893 "[2406.16893] From Transformers to LLMs: A Systematic Survey of Efficiency Considerations in NLP"
[9]: https://arxiv.org/abs/2412.19442 "[2412.19442] A Survey on Large Language Model Acceleration based on KV Cache Management"

