# Transformeræºç å®ç°ä¸ä¼˜ç§€åšå®¢


## ğŸ“˜ ä¼˜ç§€åšå®¢

* [Harvard NLP Annotated Transformerï¼ˆè¶…ç»å…¸å›¾æ–‡è®²è§£ è¶…çº§æ¨èï¼‰](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
  
* [The Illustrated Transformerï¼ˆå¯è§†åŒ–è®²è§£ï¼Œå…¥é—¨å¿…è¯»ï¼‰](https://jalammar.github.io/illustrated-transformer/)

## ğŸ§© æºç å®ç°

* [Annotated Transformerï¼ˆHarvard NLP å®˜æ–¹ä»£ç ï¼‰](https://github.com/harvardnlp/annotated-transformer)

* [Tensor2Tensorï¼ˆGoogle æ—©æœŸå®˜æ–¹å®ç°ï¼‰](https://github.com/tensorflow/tensor2tensor)
  
* [Attention Is All You Need - PyTorch å¤ç°](https://github.com/jadore801120/attention-is-all-you-need-pytorch)
  
* [HuggingFace Transformersï¼ˆä¸»æµåº“ï¼Œå·¥ä¸šçº§å®ç°ï¼‰](https://github.com/huggingface/transformers)
  
---

## 1) å…¨ç½‘é«˜è´¨é‡èµ„æ–™ç´¢å¼•ï¼ˆæŒ‰â€œåƒé€é¡ºåºâ€ï¼‰

1. **åŸè®ºæ–‡ï¼šTransformerä»0åˆ°1**
   Vaswani et al., *Attention Is All You Need*ï¼ˆæ¶æ„ã€Scaled Dot-Product Attentionã€å¤šå¤´ã€ä½ç½®ç¼–ç ã€maskç­‰éƒ½åœ¨è¿™é‡Œï¼‰([arXiv][1])
2. **é€è¡Œå®ç°ç‰ˆï¼šæŠŠè®ºæ–‡å˜æˆå¯è¿è¡Œä»£ç **ï¼ˆå¼ºçƒˆæ¨èï¼‰
   Harvard NLP *The Annotated Transformer*ï¼šæŠŠå…³é”®æ¨¡å—æ‹†å¼€ã€é…ä»£ç ä¸è§£é‡Š([nlp.seas.harvard.edu][2])
3. **æœ€å¼ºå¯è§†åŒ–ç›´è§‰ï¼šæŠŠQ/K/Vå’Œå¤šå¤´â€œçœ‹è§â€**
   Jay Alammar *The Illustrated Transformer*([jalammar.github.io][3])
4. **æ³¨æ„åŠ›æœºåˆ¶çš„â€œå‰ä¼ â€ï¼šä¸ºä»€ä¹ˆéœ€è¦attention**

   * Bahdanau additive attentionï¼šç¼“è§£â€œå›ºå®šé•¿åº¦å‘é‡ç“¶é¢ˆâ€ã€åšè½¯å¯¹é½([arXiv][4])
   * Luong attentionï¼šglobal/localã€ä¸åŒæ‰“åˆ†å‡½æ•°ä½“ç³»åŒ–æ€»ç»“([arXiv][5])
5. **Transformeré‡Œä¸¤ä¸ªâ€œè®­ç»ƒç¨³å®šæ€§ç¥å™¨â€çš„æ¥æº**

   * LayerNormï¼šä¸ä¾èµ–batchç»Ÿè®¡ã€è®­ç»ƒ/æ¨ç†ä¸€è‡´([arXiv][6])
   * Residualï¼ˆæ®‹å·®æ€æƒ³ï¼‰ï¼šæ·±ç½‘ç»œæ›´æ˜“ä¼˜åŒ–([arXiv][7])
6. **åº”ç”¨ä¾§é‡Œç¨‹ç¢‘ï¼šEncoder-onlyçš„ä»£è¡¨ï¼ˆç†è§£mask/åŒå‘çš„æ„ä¹‰ï¼‰**
   BERTï¼šæ·±åº¦åŒå‘Transformerè¡¨ç¤ºå­¦ä¹ ([arXiv][8])

---

## 2) æ³¨æ„åŠ›æœºåˆ¶ï¼šæŠŠå®ƒâ€œåƒé€â€éœ€è¦æŠ“ä½çš„3ä¸ªæœ¬è´¨

### æœ¬è´¨Aï¼šAttention = **å†…å®¹å¯»å€ï¼ˆcontent-based addressingï¼‰**

ç»™å®šä¸€ä¸ªâ€œæˆ‘ç°åœ¨è¦æ‰¾ä»€ä¹ˆâ€çš„**Query**ï¼Œå»ä¸€å †â€œå¯è¢«åŒ¹é…çš„ç´¢å¼•â€**Key**é‡Œç®—ç›¸ä¼¼åº¦ï¼Œå†å¯¹å¯¹åº”çš„â€œä¿¡æ¯å†…å®¹â€**Value**åšåŠ æƒå¹³å‡ã€‚

> ç›´è§‰ï¼šåƒåœ¨è®°å¿†åº“é‡Œç”¨Queryå»æ£€ç´¢Keyï¼Œæ£€ç´¢åˆ°çš„æƒé‡å†ç”¨æ¥æ±‡æ€»Valueã€‚

---

### æœ¬è´¨Bï¼šScaled Dot-Product Attention çš„æ•°å­¦éª¨æ¶

Transformerç”¨çš„æ˜¯ï¼ˆç¼©æ”¾ï¼‰ç‚¹ç§¯æ³¨æ„åŠ›ï¼š
[
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{mask}\right)V
]
è¿™æ˜¯è®ºæ–‡çš„æ ¸å¿ƒå…¬å¼ä¹‹ä¸€([arXiv][1])

**é€é¡¹åƒé€ï¼š**

* (QK^\top)ï¼šæ¯ä¸ªqueryå¯¹æ‰€æœ‰keyæ‰“åˆ†ï¼ˆç›¸ä¼¼åº¦çŸ©é˜µï¼Œå½¢çŠ¶é€šå¸¸æ˜¯ `T_q Ã— T_k`ï¼‰
* `softmax`ï¼šæŠŠç›¸ä¼¼åº¦å˜æˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰
* `mask`ï¼šæŠŠâ€œä¸å…è®¸çœ‹çš„ä½ç½®â€åŠ åˆ° (-\infty)ï¼Œsoftmaxåæƒé‡â‰ˆ0ï¼ˆåé¢ç»†è®²ï¼‰
* ä¹˜ (V)ï¼šå¯¹valueåšåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°â€œèåˆä¸Šä¸‹æ–‡â€çš„è¾“å‡º

**ä¸ºä»€ä¹ˆè¦é™¤ (\sqrt{d_k})**ï¼šç‚¹ç§¯éšç»´åº¦å¢å¤§æ–¹å·®å˜å¤§ï¼Œsoftmaxæ›´å®¹æ˜“é¥±å’Œï¼ˆæ¢¯åº¦å°ï¼‰ï¼Œç¼©æ”¾èƒ½ç¨³å®šè®­ç»ƒï¼›è¿™ä¹Ÿæ˜¯è®ºæ–‡æ˜ç¡®å†™å‡ºçš„è®¾è®¡ç‚¹([arXiv][1])

---

### æœ¬è´¨Cï¼šAdditive vs Dot-Productï¼ˆä½ ä¼šç»å¸¸åœ¨é¢è¯•/è®ºæ–‡é‡Œçœ‹åˆ°ï¼‰

* **Bahdanauï¼ˆadditiveï¼‰**ï¼šç”¨ä¸€ä¸ªå°MLPç®—ç›¸ä¼¼åº¦ï¼Œæ›´â€œè¡¨è¾¾åŠ›å¼ºâ€ï¼Œæ—©æœŸNMTå¸¸ç”¨([arXiv][4])
* **Dot-productï¼ˆmultiplicativeï¼‰**ï¼šçŸ©é˜µä¹˜æ³•æ›´é«˜æ•ˆã€é€‚åˆå¹¶è¡Œï¼›Transformeré€‰æ‹©å®ƒå¹¶åŠ äº†ç¼©æ”¾([arXiv][1])
  Harvardé‚£ç¯‡ä¹ŸæŠŠè¿™ä¸¤ç±»æ”¾åœ¨ä¸€èµ·å¯¹æ¯”è¿‡([nlp.seas.harvard.edu][2])

---

## 3) Transformerï¼šä½ è¦æŠŠå®ƒå½“æˆâ€œå¯é‡å¤å †å çš„ç§¯æœ¨â€

### 3.1 ä¸€å±‚Encoderé•¿ä»€ä¹ˆæ ·

ä¸€å±‚EncoderåŸºæœ¬æ˜¯ä¸¤å—ï¼š

1. **Multi-Head Self-Attention**
2. **Position-wise FFNï¼ˆé€ä½ç½®å‰é¦ˆç½‘ç»œï¼‰**
   æ¯å—å¤–é¢éƒ½æœ‰ **Residual + LayerNormï¼ˆAdd & Normï¼‰**([arXiv][1])

**å…³é”®ç‚¹ï¼šSelf-Attentioné‡Œ Qã€Kã€V éƒ½æ¥è‡ªåŒä¸€ä»½è¾“å…¥ (X)**ï¼ˆåªæ˜¯ä¹˜ä¸åŒçº¿æ€§å˜æ¢å¾—åˆ°ï¼‰ï¼š
[
Q=XW_Q,\quad K=XW_K,\quad V=XW_V
]
ï¼ˆHarvardå®ç°ç‰ˆæŠŠè¿™äº›å†™å¾—éå¸¸æ¸…æ¥šï¼‰([nlp.seas.harvard.edu][2])

---

### 3.2 Decoderä¸ºä»€ä¹ˆå¤šä¸€å—â€œCross-Attentionâ€

Decoderæ¯å±‚é€šå¸¸æ˜¯ä¸‰å—ï¼š

1. **Masked Multi-Head Self-Attention**ï¼ˆä¸èƒ½çœ‹æœªæ¥ï¼‰
2. **Cross-Attention**ï¼š**Qæ¥è‡ªdecoderå½“å‰éšçŠ¶æ€**ï¼Œ**K/Væ¥è‡ªencoderè¾“å‡º**ï¼ˆæŠŠæºåºåˆ—ä¿¡æ¯â€œå–â€è¿‡æ¥ï¼‰([arXiv][1])
3. **FFN**

è¿™å°±æ˜¯ç»å…¸encoder-decoder Transformerç¿»è¯‘æ¶æ„([arXiv][1])

---

## 4) Multi-Head Attentionï¼šä¸åªæ˜¯â€œå¤šåšå‡ æ¬¡attentionâ€

å¤šå¤´æ³¨æ„åŠ›åšçš„äº‹æ˜¯ï¼šæŠŠè¡¨ç¤ºç»´åº¦åˆ‡æˆå¤šä¸ªå­ç©ºé—´ï¼Œåœ¨ä¸åŒå­ç©ºé—´é‡Œåˆ†åˆ«åšattentionï¼Œå†æ‹¼èµ·æ¥ï¼š

[
\text{head}_i=\mathrm{Attention}(XW_Q^i, XW_K^i, XW_V^i)
]
[
\mathrm{MultiHead}(X)=\mathrm{Concat}(\text{head}_1,\dots,\text{head}_h)W_O
]
è¿™æ˜¯Transformerè¡¨è¾¾åŠ›çš„å…³é”®ä¹‹ä¸€([arXiv][1])

**ä½ çœŸæ­£è¦åƒé€çš„ç›´è§‰ï¼š**

* å•å¤´ï¼šåªèƒ½å­¦åˆ°ä¸€ç§â€œç›¸å…³æ€§åº¦é‡/èšåˆæ–¹å¼â€
* å¤šå¤´ï¼šèƒ½å¹¶è¡Œå­¦å¤šç§å…³ç³»ï¼ˆè¯­æ³•ä¾èµ–ã€æŒ‡ä»£ã€ä¸»é¢˜ä¸€è‡´æ€§â€¦ï¼‰ï¼Œæœ€åèåˆ

---

## 5) Maskï¼šTransformeré‡Œæœ€å®¹æ˜“â€œçœ‹æ‡‚ä½†å†™é”™â€çš„åœ°æ–¹

ä½ è‡³å°‘è¦åŒºåˆ†ä¸¤ç§maskï¼š

1. **Padding maskï¼ˆå¡«å……maskï¼‰**
   æŠŠpaddingä½ç½®å±è”½æ‰ï¼Œå¦åˆ™æ¨¡å‹ä¼šæŠŠâ€œPADâ€ä¹Ÿå½“æˆä¿¡æ¯ã€‚Harvardå®ç°é‡Œæœ‰å¾ˆæ¸…æ™°çš„maskæ„é€ æ–¹å¼([nlp.seas.harvard.edu][2])

2. **Causal / Subsequent maskï¼ˆå› æœmaskï¼‰**
   ç”¨åœ¨Decoder self-attentionï¼šä½ç½®tåªèƒ½çœ‹ (\le t) çš„tokenï¼Œä¿è¯è‡ªå›å½’ç”Ÿæˆçš„å› æœæ€§([arXiv][1])

---

## 6) ä½ç½®ç¼–ç ï¼šæ²¡æœ‰RNNä»¥åï¼Œâ€œé¡ºåºâ€ä»å“ªæ¥ï¼Ÿ

Transformeræ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæ‰€ä»¥å¿…é¡»æ˜¾å¼æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚åŸè®ºæ–‡ç”¨çš„æ˜¯**æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç **ï¼ˆä¹Ÿå¯å­¦ä¹ ä½ç½®embeddingï¼‰([arXiv][1])

ä½ è¦æŠ“ä½çš„è¦ç‚¹ï¼š

* **Self-attentionæœ¬èº«æ˜¯ç½®æ¢ä¸å˜çš„**ï¼ˆæ‰“ä¹±tokené¡ºåºï¼Œæ³¨æ„åŠ›è®¡ç®—å½¢å¼ä¸å˜ï¼‰ï¼Œæ‰€ä»¥å¿…é¡»åŠ ä½ç½®
* ä½ç½®ç¼–ç ç­‰ä»·äºå‘Šè¯‰æ¨¡å‹â€œç¬¬å‡ ä¸ªtokenâ€ï¼Œè®©æ³¨æ„åŠ›èƒ½è¡¨è¾¾ç›¸å¯¹/ç»å¯¹é¡ºåºå…³ç³»

---

## 7) ä½ æƒ³â€œå®Œå…¨åƒé€â€ï¼šç»™ä½ ä¸€å¥—æœ€æœ‰æ•ˆçš„è®­ç»ƒæ–¹å¼ï¼ˆä¸é æ­»è®°ï¼‰

### 7.1 6ä¸ªå¿…é¡»èƒ½æ‰‹å†™/å£è¿°çš„â€œæ£€æŸ¥ç‚¹â€

1. å†™å‡ºå¹¶è§£é‡Š attention å…¬å¼ï¼ˆå«maskï¼‰([arXiv][1])
2. è¯´æ¸… self-attn vs cross-attn çš„Q/K/Væ¥æº([nlp.seas.harvard.edu][2])
3. è¯´æ¸… multi-head çš„â€œä¸ºä»€ä¹ˆä¸æ˜¯å¤šæ­¤ä¸€ä¸¾â€([NeurIPS Proceedings][9])
4. è¯´æ¸…ä¸¤ç§maskå„è‡ªè§£å†³ä»€ä¹ˆé—®é¢˜([nlp.seas.harvard.edu][2])
5. è¯´æ¸… residual + layernorm ä¸ºä»€ä¹ˆèƒ½ç¨³å®šè®­ç»ƒ([nlp.seas.harvard.edu][2])
6. è¯´æ¸…ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ã€åŸè®ºæ–‡æ€ä¹ˆåš([arXiv][1])

### 7.2 3ä¸ªâ€œåšå®Œå°±ä¼šäº†â€çš„å®æˆ˜ä»»åŠ¡

* **ä»»åŠ¡1ï¼šä»é›¶å®ç°Scaled Dot-Product Attentionï¼ˆå¸¦maskï¼‰**
  è¾“å…¥éšæœºQ/K/Vï¼Œæ£€æŸ¥ï¼šmaskä½ç½®æƒé‡â‰ˆ0ï¼›softmaxæ¯è¡Œå’Œä¸º1ã€‚å‚è€ƒHarvardé€è¡Œå®ç°([nlp.seas.harvard.edu][2])
* **ä»»åŠ¡2ï¼šå®ç°Multi-Head Attentionå¹¶åšå½¢çŠ¶è‡ªæ£€**
  å¼ºåˆ¶è‡ªå·±å†™å‡ºæ¯ä¸€æ­¥å¼ é‡å½¢çŠ¶ï¼ˆ`B,T,d_model` â†’ `B,h,T,d_k` ç­‰ï¼‰ï¼Œæœ€èƒ½æ²»â€œçœ‹æ‡‚ä½†å†™ä¸å‡ºæ¥â€ã€‚([nlp.seas.harvard.edu][2])
* **ä»»åŠ¡3ï¼šè·‘ä¸€ä¸ªæœ€å°Transformer**
  ç”¨æå°æ•°æ®ï¼ˆcopy task / reverse taskï¼‰è®­ç»ƒåˆ°è¿‡æ‹Ÿåˆï¼›è¿™ä¼šè®©ä½ çœŸæ­£ç†è§£maskã€ä½ç½®ã€decoderè¾“å…¥å³ç§»ç­‰ç»†èŠ‚ã€‚Harvardé‚£ç¯‡å°±æœ‰è®­ç»ƒè„šæ‰‹æ¶æ€è·¯([nlp.seas.harvard.edu][2])

---

## 8) Transformerå®¶æ—ä¸€çœ¼çœ‹æ‡‚ï¼šä½ åœ¨å­¦çš„åˆ°åº•æ˜¯å“ªä¸€ç§ï¼Ÿ

* **Encoder-Decoder**ï¼šæœºå™¨ç¿»è¯‘/Seq2Seqï¼ˆåŸè®ºæ–‡ï¼‰([arXiv][1])
* **Encoder-only**ï¼šç†è§£å‹ä»»åŠ¡ï¼ˆBERTæ˜¯ä»£è¡¨ï¼‰([arXiv][8])
* **Decoder-only**ï¼šè‡ªå›å½’ç”Ÿæˆï¼ˆå¾ˆå¤šLLMå±äºè¿™ç±»ï¼›æ ¸å¿ƒæ˜¯causal maskï¼‰([arXiv][1])

---
[1]: https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com "Attention Is All You Need"
[2]: https://nlp.seas.harvard.edu/annotated-transformer/?utm_source=chatgpt.com "The Annotated Transformer - Harvard University"
[3]: https://jalammar.github.io/illustrated-transformer/?utm_source=chatgpt.com "The Illustrated Transformer â€“ Jay Alammar â€“ Visualizing machine ..."
[4]: https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com "Neural Machine Translation by Jointly Learning to Align and Translate"
[5]: https://arxiv.org/abs/1508.04025?utm_source=chatgpt.com "Effective Approaches to Attention-based Neural Machine Translation"
[6]: https://arxiv.org/abs/1607.06450?utm_source=chatgpt.com "Layer Normalization"
[7]: https://arxiv.org/abs/1512.03385?utm_source=chatgpt.com "Deep Residual Learning for Image Recognition"
[8]: https://arxiv.org/abs/1810.04805?utm_source=chatgpt.com "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
[9]: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf?utm_source=chatgpt.com "Attention Is All You Need - NeurIPS"
